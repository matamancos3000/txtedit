import os
import re
import csv
from pathlib import Path
from bs4 import BeautifulSoup

# ==============================
# CONFIGURACIÓN
# ==============================
CARPETA_RAIZ = Path(r"../calidad2025")  # Carpeta raíz con TODOS los instructivos
URL_SHAREPOINT_BASE = "https://teams.global.hsbc/sites/labpspyas-2019/CALIDAD20251"
LOG_CSV = CARPETA_RAIZ / "../log_cambios.csv"

# Extensiones de office/pdf
EXT_RECURSOS = (".docx", ".pdf", ".xlsx", ".pptx", ".doc", ".xls", ".xlsm")

# ==============================
# FUNCIONES
# ==============================

def registrar_cambio(log_writer, archivo_html, tipo, original, nuevo):
    """Registra los cambios de enlaces en el log."""
    try:
        log_writer.writerow([archivo_html, tipo, original, nuevo])
    except Exception as e:
        print(f"[WARN] No se pudo registrar un cambio en {archivo_html}: {e}")

def convertir_enlaces(html_path, log_writer):
    """
    Convierte los enlaces dentro de un archivo .htm/.html
    según las reglas de SharePoint.
    """
    with open(html_path, "r", encoding="utf-8", errors="ignore") as f:
        soup = BeautifulSoup(f, "html.parser")

    cambios = False

    for link in soup.find_all(href=True):
        original = link["href"]

        # --- Regla 1: enlaces internos a instructivos .htm → SharePoint .pdf
        if original.lower().endswith((".htm", ".html")):
            if not original.startswith("http"):  # solo links relativos
                # Construir la ruta relativa desde la raíz
                relativo = (html_path.parent / original).resolve().relative_to(CARPETA_RAIZ.resolve())
                nuevo = f"{URL_SHAREPOINT_BASE}/{relativo.as_posix()}"
                nuevo = re.sub(r"\.htm[l]?$", ".pdf", nuevo, flags=re.IGNORECASE)

                link["href"] = nuevo
                registrar_cambio(log_writer, html_path, "HTML->PDF", original, nuevo)
                cambios = True

        # --- Regla 2: enlaces a recursos de Formatos2021 desde 'calidad2021/formato2021/'
        elif "calidad2021/formato2021/" in original.lower():
            # Extraer la parte después de 'formato2021/'
            match = re.search(r"calidad2021/formato2021/(.*)", original, re.IGNORECASE)
            if match:
                relativo = match.group(1)
                nuevo = f"{URL_SHAREPOINT_BASE}/Formatos2021/{relativo}"
                link["href"] = nuevo
                registrar_cambio(log_writer, html_path, "RECURSO", original, nuevo)
                cambios = True

    # Guardar cambios solo si hubo modificaciones
    if cambios:
        with open(html_path, "w", encoding="utf-8", errors="ignore") as f:
            f.write(str(soup))


def procesar_carpeta(carpeta_raiz):
    """
    Recorre toda la carpeta raíz y procesa todos los archivos .htm/.html,
    respetando las carpetas *_archivos (no se tocan).
    """
    procesados = set()

    with open(LOG_CSV, "w", newline="", encoding="utf-8") as log_file:
        log_writer = csv.writer(log_file)
        log_writer.writerow(["Archivo HTML", "Tipo", "Enlace original", "Enlace nuevo"])  # sin fecha

        for html_path in carpeta_raiz.rglob("*.[hH][tT][mM]*"):
            # Saltar carpetas de recursos *_archivos
            if any("_archivos" in part for part in html_path.parts):
                continue

            # Evitar procesar el mismo archivo dos veces
            if html_path in procesados:
                continue

            try:
                convertir_enlaces(html_path, log_writer)
                procesados.add(html_path)
                print(f"[OK] Procesado: {html_path}")
            except Exception as e:
                print(f"[ERROR] No se pudo procesar {html_path}: {e}")
                continue


# ==============================
# EJECUCIÓN
# ==============================
if __name__ == "__main__":
    procesar_carpeta(CARPETA_RAIZ)
    print("✅ Proceso completado. Revisa el log:", LOG_CSV)
